{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro-md",
            "metadata": {},
            "source": [
                "# üéôÔ∏è Hindi Disfluency Restoration Pipeline\n",
                "\n",
                "This notebook restores disfluencies (filler words like \"‡§π‡§Æ‡•ç‡§Æ\", \"‡§π‡§æ‡§Ç\", \"‡§â‡§Æ‡•ç‡§Æ\") to clean Hindi transcripts using:\n",
                "\n",
                "1. **Whisper ASR** - Transcribes audio to detect spoken disfluencies\n",
                "2. **Sequence Alignment** - Compares clean text with ASR output to find insertions\n",
                "3. **N-gram Language Model** - Validates that insertions sound natural\n",
                "4. **Position Prior** - Disfluencies usually occur at the start of utterances\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-md",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# CONFIGURATION\n",
                "# =============================================================================\n",
                "# These paths point to the competition data on Kaggle\n",
                "\n",
                "AUDIO_DIR = \"/kaggle/input/nppe-2-automatic-disfluency-restoration/downloaded_audios\"  # Audio files (.wav)\n",
                "TEST_CSV = \"/kaggle/input/nppe-2-automatic-disfluency-restoration/test.csv\"            # Test set to process\n",
                "TRAIN_CSV = \"/kaggle/input/nppe-2-automatic-disfluency-restoration/train.csv\"          # Training data for LM\n",
                "DISF_CSV_PATH = \"/kaggle/input/nppe-2-automatic-disfluency-restoration/unique_disfluencies.csv\"  # Disfluency list\n",
                "OUTPUT_DIR = \"/kaggle/working/\"  # Where to save results\n",
                "\n",
                "# Verbose logging - set to True for detailed output\n",
                "VERBOSE = True\n",
                "\n",
                "def log(msg):\n",
                "    \"\"\"Print message only if VERBOSE is True\"\"\"\n",
                "    if VERBOSE:\n",
                "        print(f\"[LOG] {msg}\")\n",
                "\n",
                "print(\"‚úÖ Configuration loaded\")\n",
                "print(f\"   Audio directory: {AUDIO_DIR}\")\n",
                "print(f\"   Output directory: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# IMPORTS\n",
                "# =============================================================================\n",
                "# Standard library\n",
                "import os              # File path operations\n",
                "import re              # Regular expressions for text cleaning\n",
                "import gc              # Garbage collection to free memory\n",
                "import pickle          # Save/load cache to disk\n",
                "import unicodedata     # Normalize Hindi text (NFKC form)\n",
                "from difflib import SequenceMatcher  # Find differences between two sequences\n",
                "from collections import Counter      # Count n-gram frequencies\n",
                "\n",
                "# Data science\n",
                "import pandas as pd    # DataFrames for CSV handling\n",
                "import numpy as np     # Numerical operations\n",
                "\n",
                "# Deep learning\n",
                "import torch           # PyTorch for GPU acceleration\n",
                "from transformers import WhisperProcessor, WhisperForConditionalGeneration  # Whisper ASR model\n",
                "import librosa         # Audio loading and processing\n",
                "\n",
                "# Evaluation\n",
                "from jiwer import wer  # Word Error Rate metric\n",
                "\n",
                "print(\"‚úÖ All imports successful\")\n",
                "print(f\"   PyTorch version: {torch.__version__}\")\n",
                "print(f\"   CUDA available: {torch.cuda.is_available()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "disf-md",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Disfluency Set & Thresholds\n",
                "\n",
                "Disfluencies are filler words people say when thinking (\"umm\", \"uh\", etc.).\n",
                "In Hindi, common ones include:\n",
                "- **‡§π‡§Æ‡•ç‡§Æ** (hmm)\n",
                "- **‡§π‡§æ‡§Ç/‡§π‡§æ‡§Å** (yes, often used as filler)\n",
                "- **‡§â‡§Æ‡•ç‡§Æ** (umm)\n",
                "- **‡§§‡•ã/‡§µ‡•ã** (so/that - can be filler or real word)\n",
                "\n",
                "Each disfluency has its own **confidence threshold** - words that could be real words (like \"‡§π‡§æ‡§Ç\" = yes) need higher ASR confidence to be inserted."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "disfluency-set",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# DISFLUENCY SET\n",
                "# =============================================================================\n",
                "\n",
                "def norm(x):\n",
                "    \"\"\"\n",
                "    Normalize text for consistent comparison.\n",
                "    - NFKC normalization handles different Unicode representations\n",
                "    - Lowercase for case-insensitive matching\n",
                "    \"\"\"\n",
                "    return unicodedata.normalize('NFKC', str(x).strip().lower())\n",
                "\n",
                "# Load disfluencies from CSV file\n",
                "try:\n",
                "    disf_df = pd.read_csv(DISF_CSV_PATH)\n",
                "    DISFLUENCY_SET = set(norm(x) for x in disf_df['disfluency'].astype(str).tolist() if x and str(x).strip())\n",
                "    log(f\"Loaded {len(DISFLUENCY_SET)} disfluencies from CSV\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è Could not load disfluency CSV: {e}\")\n",
                "    DISFLUENCY_SET = set()\n",
                "\n",
                "# Add common Hindi fillers that might not be in the CSV\n",
                "COMMON_FILLERS = {\n",
                "    '‡§Ö‡§Ç', '‡§â‡§Ç', '‡§ä‡§Ç', '‡§Ü‡§Ç', '‡§è‡§Ç', '‡§ì‡§Ç',  # Short vowel sounds\n",
                "    '‡§π‡§Æ‡•ç‡§Æ', '‡§π‡§æ‡§Ç', '‡§π‡§æ‡§Å',                 # Hmm, yes\n",
                "    '‡§â‡§Æ‡•ç‡§Æ', '‡§Ö‡§Æ‡•ç‡§Æ',                       # Umm\n",
                "    '‡§π', '‡§Ö', '‡§è',                        # Single-letter fillers\n",
                "    '‡§§‡•ã', '‡§µ‡•ã', '‡§ú‡•ã',                     # Conjunctions often used as fillers\n",
                "    '‡§Æ‡§§‡§≤‡§¨', '‡§¨‡§∏', '‡§Ö‡§ö‡•ç‡§õ‡§æ'                 # \"I mean\", \"just\", \"okay\"\n",
                "}\n",
                "DISFLUENCY_SET |= set(norm(x) for x in COMMON_FILLERS)\n",
                "\n",
                "# Build regex pattern for removing disfluencies (longest match first)\n",
                "# This prevents partial matches (e.g., \"‡§π‡§æ‡§Ç\" matching inside a longer word)\n",
                "pattern = r'\\b(?:' + '|'.join(re.escape(x) for x in sorted(DISFLUENCY_SET, key=len, reverse=True)) + r')\\b'\n",
                "RE_DISF = re.compile(pattern, flags=re.IGNORECASE)\n",
                "\n",
                "print(f\"‚úÖ Loaded {len(DISFLUENCY_SET)} total disfluencies\")\n",
                "print(f\"   Sample: {list(DISFLUENCY_SET)[:8]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "thresholds",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# PER-DISFLUENCY CONFIDENCE THRESHOLDS\n",
                "# =============================================================================\n",
                "# Each disfluency has its own threshold based on how often it's used as a real word.\n",
                "# More negative = more lenient (insert even with low confidence)\n",
                "# Less negative = stricter (only insert if ASR is very confident)\n",
                "\n",
                "DISFLUENCY_THRESHOLDS = {\n",
                "    # Very common fillers - be lenient\n",
                "    '‡§π‡§Æ‡•ç‡§Æ': -8.0,   # \"Hmm\" - almost always a filler\n",
                "    '‡§â‡§Æ‡•ç‡§Æ': -7.0,   # \"Umm\" - almost always a filler\n",
                "    '‡§Ö‡§Ç': -7.0,     # Short sound - usually a filler\n",
                "    '‡§π': -7.0,      # Single letter filler\n",
                "    '‡§Ö‡§π': -7.0,     # \"Ah\"\n",
                "    '‡§â‡§π': -7.0,     # \"Uh\"\n",
                "    '‡§ì': -6.0,      # Can be interjection\n",
                "    \n",
                "    # Words that can be real - be stricter\n",
                "    '‡§π‡§æ‡§Ç': -5.0,    # \"Yes\" - could be real acknowledgment\n",
                "    '‡§π‡§æ‡§Å': -5.0,    # Same as above (different Unicode)\n",
                "    '‡§§‡•ã': -4.0,     # \"So\" - often a real conjunction\n",
                "    '‡§µ‡•ã': -4.0,     # \"That\" - often a real pronoun\n",
                "    '‡§î‡§∞': -3.0,     # \"And\" - almost always a real word\n",
                "    \n",
                "    'default': -6.0  # Default for unknown disfluencies\n",
                "}\n",
                "\n",
                "def get_disfluency_threshold(token):\n",
                "    \"\"\"Get the confidence threshold for a specific disfluency.\"\"\"\n",
                "    norm_token = norm(token)\n",
                "    return DISFLUENCY_THRESHOLDS.get(norm_token, DISFLUENCY_THRESHOLDS['default'])\n",
                "\n",
                "print(\"‚úÖ Disfluency thresholds configured\")\n",
                "print(\"   Lenient: ‡§π‡§Æ‡•ç‡§Æ (-8.0), ‡§â‡§Æ‡•ç‡§Æ (-7.0)\")\n",
                "print(\"   Strict: ‡§π‡§æ‡§Ç (-5.0), ‡§§‡•ã (-4.0), ‡§î‡§∞ (-3.0)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "asr-md",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Whisper ASR Model\n",
                "\n",
                "We use **Whisper Large V3** fine-tuned on Hindi data by ARTPARK-IISc.\n",
                "The model:\n",
                "1. Takes audio as input\n",
                "2. Outputs Hindi text transcription\n",
                "3. Provides **confidence scores** for each token (log probabilities)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load-model",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# LOAD WHISPER MODEL\n",
                "# =============================================================================\n",
                "\n",
                "print(\"üîÑ Loading Whisper model (this may take 1-2 minutes)...\")\n",
                "\n",
                "# Detect if GPU is available\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"   Using device: {device}\")\n",
                "\n",
                "# Model ID - ARTPARK's Hindi-tuned Whisper\n",
                "model_id = \"ARTPARK-IISc/whisper-large-v3-vaani-hindi\"\n",
                "\n",
                "# Load processor (handles audio preprocessing and text decoding)\n",
                "processor = WhisperProcessor.from_pretrained(model_id)\n",
                "log(\"Processor loaded\")\n",
                "\n",
                "# Load model with memory optimizations\n",
                "model = WhisperForConditionalGeneration.from_pretrained(\n",
                "    model_id,\n",
                "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,  # FP16 on GPU\n",
                "    low_cpu_mem_usage=True  # Load weights incrementally\n",
                ")\n",
                "model.to(device)  # Move to GPU\n",
                "model.eval()      # Set to inference mode (no dropout)\n",
                "torch.set_grad_enabled(False)  # Disable gradient computation (saves memory)\n",
                "\n",
                "# Force Hindi language output\n",
                "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"hi\", task=\"transcribe\")\n",
                "\n",
                "# Clean up memory\n",
                "gc.collect()\n",
                "if device == \"cuda\":\n",
                "    torch.cuda.empty_cache()\n",
                "\n",
                "print(f\"‚úÖ Model loaded successfully on {device}\")\n",
                "if device == \"cuda\":\n",
                "    print(f\"   GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "text-md",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Text Processing Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "text-utils",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# TEXT PROCESSING FUNCTIONS\n",
                "# =============================================================================\n",
                "\n",
                "def normalize_text(text):\n",
                "    \"\"\"\n",
                "    Normalize text for alignment comparison.\n",
                "    - Removes punctuation (‡•§ ‡•• , . ! ? etc.)\n",
                "    - Collapses multiple spaces\n",
                "    - Lowercases everything\n",
                "    \"\"\"\n",
                "    text = unicodedata.normalize('NFKC', str(text))\n",
                "    text = re.sub(r'[‡•§‡••,.!?;:\\'\"()\\[\\]-]+', ' ', text)  # Remove punctuation\n",
                "    return re.sub(r'\\s+', ' ', text).strip().lower()\n",
                "\n",
                "def tokenize(text):\n",
                "    \"\"\"Split text into words (tokens) for alignment.\"\"\"\n",
                "    return [t for t in normalize_text(text).split() if t]\n",
                "\n",
                "def make_clean(text):\n",
                "    \"\"\"\n",
                "    Remove all disfluencies from text to create a 'clean' version.\n",
                "    Used when we need to compare clean vs. ASR output.\n",
                "    \"\"\"\n",
                "    if pd.isna(text) or not isinstance(text, str):\n",
                "        return \"\"\n",
                "    t = unicodedata.normalize('NFKC', text)\n",
                "    t = RE_DISF.sub(' ', t)  # Remove disfluencies\n",
                "    return re.sub(r'\\s+', ' ', t).strip()\n",
                "\n",
                "def is_disfluency(token):\n",
                "    \"\"\"Check if a token is a known disfluency.\"\"\"\n",
                "    return norm(token) in DISFLUENCY_SET\n",
                "\n",
                "def is_repetition(token, context_tokens, position):\n",
                "    \"\"\"\n",
                "    Check if token is a repetition of adjacent word.\n",
                "    E.g., \"‡§Æ‡•à‡§Ç ‡§Æ‡•à‡§Ç\" (I I) - the second \"‡§Æ‡•à‡§Ç\" is a disfluent repetition.\n",
                "    \"\"\"\n",
                "    # Check if same as word at current position\n",
                "    if position < len(context_tokens) and token == context_tokens[position]:\n",
                "        return True\n",
                "    # Check if same as previous word\n",
                "    if position > 0 and position <= len(context_tokens) and token == context_tokens[position - 1]:\n",
                "        return True\n",
                "    return False\n",
                "\n",
                "print(\"‚úÖ Text processing utilities ready\")\n",
                "\n",
                "# Demo\n",
                "demo_text = \"‡§π‡§Æ‡•ç‡§Æ, ‡§Æ‡•à‡§Ç ‡§∏‡•ã‡§ö‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§ï‡§ø ‡§Ø‡§π ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à‡•§\"\n",
                "print(f\"\\n   Demo input: {demo_text}\")\n",
                "print(f\"   Normalized: {normalize_text(demo_text)}\")\n",
                "print(f\"   Clean (no disfluencies): {make_clean(demo_text)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "asr-func-md",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Audio Transcription with Confidence Scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "transcribe",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# TOKEN-TO-WORD MAPPING\n",
                "# =============================================================================\n",
                "\n",
                "def map_tokens_to_words(token_ids, token_logprobs, tokenizer, decoded_text):\n",
                "    \"\"\"\n",
                "    Map sub-word token log-probabilities to word-level confidence.\n",
                "    \n",
                "    Whisper uses sub-word tokenization (e.g., \"‡§ï‡§π‡§æ‡§®‡•Ä\" might be split into\n",
                "    \"‡§ï‡§π‡§æ\" + \"‡§®‡•Ä\"). This function groups tokens back into words and averages\n",
                "    their log-probabilities.\n",
                "    \n",
                "    Returns: List of {'word': str, 'avg_logprob': float}\n",
                "    \"\"\"\n",
                "    words = decoded_text.strip().split()\n",
                "    if not words or not token_ids:\n",
                "        return [{'word': w, 'avg_logprob': None} for w in words]\n",
                "    \n",
                "    # Decode each token ID to its text representation\n",
                "    token_texts = []\n",
                "    for tid in token_ids:\n",
                "        try:\n",
                "            txt = tokenizer.decode([tid], skip_special_tokens=True)\n",
                "            token_texts.append(txt)\n",
                "        except:\n",
                "            token_texts.append(\"\")\n",
                "    \n",
                "    # Match tokens to words\n",
                "    word_infos = []\n",
                "    token_idx = 0\n",
                "    current_text = \"\"\n",
                "    current_logprobs = []\n",
                "    \n",
                "    for word in words:\n",
                "        word_norm = normalize_text(word)\n",
                "        \n",
                "        # Consume tokens until we've matched this word\n",
                "        while token_idx < len(token_texts):\n",
                "            current_text += token_texts[token_idx].strip()\n",
                "            if token_idx < len(token_logprobs):\n",
                "                current_logprobs.append(token_logprobs[token_idx])\n",
                "            token_idx += 1\n",
                "            \n",
                "            current_norm = normalize_text(current_text)\n",
                "            if word_norm in current_norm or current_norm == word_norm:\n",
                "                break\n",
                "            if len(current_norm) >= len(word_norm) * 2:\n",
                "                break  # Mismatch, move on\n",
                "        \n",
                "        # Calculate average log-probability for this word\n",
                "        avg_lp = float(np.mean(current_logprobs)) if current_logprobs else None\n",
                "        word_infos.append({'word': word, 'avg_logprob': avg_lp})\n",
                "        current_text = \"\"\n",
                "        current_logprobs = []\n",
                "    \n",
                "    return word_infos\n",
                "\n",
                "print(\"‚úÖ Token-to-word mapping function ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "transcribe-audio",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# AUDIO TRANSCRIPTION\n",
                "# =============================================================================\n",
                "\n",
                "def transcribe_audio(audio_path, max_length=448, chunk_s=30):\n",
                "    \"\"\"\n",
                "    Transcribe audio file using Whisper with per-word confidence scores.\n",
                "    \n",
                "    Args:\n",
                "        audio_path: Path to .wav file\n",
                "        max_length: Maximum output tokens per chunk\n",
                "        chunk_s: Audio chunk size in seconds (30s is Whisper's native size)\n",
                "    \n",
                "    Returns:\n",
                "        text: Full transcription\n",
                "        tokens_info: List of {'word': str, 'avg_logprob': float}\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Load audio at 16kHz (Whisper's expected sample rate)\n",
                "        audio, sr = librosa.load(audio_path, sr=16000)\n",
                "        chunk_len = chunk_s * sr  # Samples per chunk\n",
                "        \n",
                "        all_texts = []\n",
                "        all_tokens = []\n",
                "        \n",
                "        # Process audio in chunks (handles long recordings)\n",
                "        for i in range(0, len(audio), chunk_len):\n",
                "            chunk = audio[i:i+chunk_len]\n",
                "            \n",
                "            # Convert audio to model input features\n",
                "            inputs = processor(chunk, sampling_rate=16000, return_tensors=\"pt\")\n",
                "            input_features = inputs.input_features.to(device)\n",
                "            if device == \"cuda\":\n",
                "                input_features = input_features.half()  # FP16 for GPU\n",
                "            \n",
                "            # Generate transcription with scores\n",
                "            with torch.no_grad():\n",
                "                out = model.generate(\n",
                "                    input_features,\n",
                "                    forced_decoder_ids=forced_decoder_ids,\n",
                "                    max_length=max_length,\n",
                "                    return_dict_in_generate=True,\n",
                "                    output_scores=True  # Get token log-probabilities\n",
                "                )\n",
                "            \n",
                "            # Decode output tokens to text\n",
                "            seq = out.sequences[0]\n",
                "            decoded = processor.batch_decode(seq.unsqueeze(0), skip_special_tokens=True)[0]\n",
                "            all_texts.append(decoded.strip())\n",
                "            \n",
                "            # Extract per-token log-probabilities\n",
                "            scores = out.scores\n",
                "            if scores:\n",
                "                token_ids = seq.tolist()\n",
                "                token_logprobs = []\n",
                "                for idx, step_logits in enumerate(scores):\n",
                "                    logp = torch.log_softmax(step_logits, dim=-1)\n",
                "                    if idx + 1 < len(token_ids):\n",
                "                        chosen_id = token_ids[idx + 1]\n",
                "                        token_logprobs.append(float(logp[0, chosen_id].cpu().numpy()))\n",
                "                \n",
                "                word_infos = map_tokens_to_words(token_ids, token_logprobs, processor.tokenizer, decoded)\n",
                "                all_tokens.extend(word_infos)\n",
                "            else:\n",
                "                # Fallback: no confidence scores available\n",
                "                for w in decoded.strip().split():\n",
                "                    all_tokens.append({'word': w, 'avg_logprob': None})\n",
                "            \n",
                "            del input_features, out\n",
                "        \n",
                "        # Clean up GPU memory\n",
                "        gc.collect()\n",
                "        if device == \"cuda\":\n",
                "            torch.cuda.empty_cache()\n",
                "        \n",
                "        return \" \".join(all_texts).strip(), all_tokens\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Transcription error: {e}\")\n",
                "        gc.collect()\n",
                "        return \"\", []\n",
                "\n",
                "print(\"‚úÖ Transcription function ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cache-md",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ ASR Cache (Avoid Re-transcribing)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cache",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# ASR CACHE\n",
                "# =============================================================================\n",
                "# Transcription is slow (~5-10s per audio). Cache results to avoid redoing work.\n",
                "\n",
                "CACHE_PATH = os.path.join(OUTPUT_DIR, \"asr_cache.pkl\")\n",
                "ASR_CACHE = {}\n",
                "\n",
                "def load_cache():\n",
                "    \"\"\"Load previously cached ASR results.\"\"\"\n",
                "    global ASR_CACHE\n",
                "    if os.path.exists(CACHE_PATH):\n",
                "        with open(CACHE_PATH, 'rb') as f:\n",
                "            ASR_CACHE = pickle.load(f)\n",
                "        print(f\"üìÇ Loaded {len(ASR_CACHE)} cached ASR results\")\n",
                "    else:\n",
                "        print(\"üìÇ No cache found, starting fresh\")\n",
                "\n",
                "def save_cache():\n",
                "    \"\"\"Save ASR cache to disk.\"\"\"\n",
                "    with open(CACHE_PATH, 'wb') as f:\n",
                "        pickle.dump(ASR_CACHE, f)\n",
                "    log(f\"Cache saved ({len(ASR_CACHE)} entries)\")\n",
                "\n",
                "def get_asr_transcript(audio_path):\n",
                "    \"\"\"\n",
                "    Get ASR transcription, using cache if available.\n",
                "    Returns: (text, tokens_info)\n",
                "    \"\"\"\n",
                "    audio_id = os.path.splitext(os.path.basename(audio_path))[0]\n",
                "    \n",
                "    # Check cache\n",
                "    if audio_id in ASR_CACHE:\n",
                "        cached = ASR_CACHE[audio_id]\n",
                "        if isinstance(cached, dict):\n",
                "            return cached.get('text', ''), cached.get('tokens', [])\n",
                "        return cached, []\n",
                "    \n",
                "    # Transcribe and cache\n",
                "    log(f\"Transcribing {audio_id}...\")\n",
                "    text, tokens = transcribe_audio(audio_path)\n",
                "    ASR_CACHE[audio_id] = {'text': text, 'tokens': tokens}\n",
                "    return text, tokens\n",
                "\n",
                "print(\"‚úÖ Cache system ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "lm-md",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ N-Gram Language Model\n",
                "\n",
                "The language model checks if an insertion sounds natural.\n",
                "- Built from training transcripts\n",
                "- Uses trigrams (3-word sequences)\n",
                "- Assigns probability to word sequences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ngram",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# N-GRAM LANGUAGE MODEL\n",
                "# =============================================================================\n",
                "\n",
                "NGRAM_COUNTS = None   # Stores (w1, w2, w3) -> count\n",
                "PREFIX_COUNTS = None  # Stores (w1, w2) -> count\n",
                "NGRAM_VOCAB_SIZE = 0\n",
                "\n",
                "def build_ngram_model(n=3):\n",
                "    \"\"\"\n",
                "    Build n-gram language model from training transcripts.\n",
                "    Uses add-alpha smoothing for unseen n-grams.\n",
                "    \"\"\"\n",
                "    global NGRAM_COUNTS, PREFIX_COUNTS, NGRAM_VOCAB_SIZE\n",
                "    \n",
                "    print(\"üîÑ Building n-gram language model from training data...\")\n",
                "    \n",
                "    try:\n",
                "        train_df = pd.read_csv(TRAIN_CSV)\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Could not load training data: {e}\")\n",
                "        return\n",
                "    \n",
                "    ngram_counts = Counter()\n",
                "    prefix_counts = Counter()\n",
                "    \n",
                "    # Count n-grams in all transcripts\n",
                "    for text in train_df['transcript'].dropna():\n",
                "        tokens = ['<s>'] + tokenize(str(text)) + ['</s>']  # Add start/end markers\n",
                "        for i in range(len(tokens) - n + 1):\n",
                "            ngram = tuple(tokens[i:i+n])\n",
                "            prefix = ngram[:-1]\n",
                "            ngram_counts[ngram] += 1\n",
                "            prefix_counts[prefix] += 1\n",
                "    \n",
                "    NGRAM_COUNTS = ngram_counts\n",
                "    PREFIX_COUNTS = prefix_counts\n",
                "    NGRAM_VOCAB_SIZE = len(set(t for ng in ngram_counts for t in ng))\n",
                "    \n",
                "    print(f\"‚úÖ Built LM: {len(ngram_counts):,} n-grams, vocab size {NGRAM_VOCAB_SIZE:,}\")\n",
                "\n",
                "def sentence_logprob(tokens, n=3, alpha=0.1):\n",
                "    \"\"\"\n",
                "    Compute log-probability of a sentence under the n-gram model.\n",
                "    Uses add-alpha (Laplace) smoothing for unseen n-grams.\n",
                "    \"\"\"\n",
                "    if NGRAM_COUNTS is None:\n",
                "        return 0.0\n",
                "    \n",
                "    tokens = ['<s>'] + list(tokens) + ['</s>']\n",
                "    logprob = 0.0\n",
                "    V = max(1, NGRAM_VOCAB_SIZE)\n",
                "    \n",
                "    for i in range(len(tokens) - n + 1):\n",
                "        ngram = tuple(tokens[i:i+n])\n",
                "        prefix = ngram[:-1]\n",
                "        count = NGRAM_COUNTS.get(ngram, 0)\n",
                "        prefix_count = PREFIX_COUNTS.get(prefix, 0)\n",
                "        prob = (count + alpha) / (prefix_count + alpha * V + 1e-10)\n",
                "        logprob += np.log(prob + 1e-10)\n",
                "    \n",
                "    return logprob\n",
                "\n",
                "def check_insertion_plausibility(clean_tokens, position, token, lm_threshold=-2.0):\n",
                "    \"\"\"\n",
                "    Check if inserting a token maintains sentence plausibility.\n",
                "    Returns True if log-prob doesn't drop too much after insertion.\n",
                "    \"\"\"\n",
                "    if NGRAM_COUNTS is None:\n",
                "        return True\n",
                "    \n",
                "    logprob_before = sentence_logprob(clean_tokens)\n",
                "    tokens_with = list(clean_tokens)\n",
                "    tokens_with.insert(min(position, len(tokens_with)), token)\n",
                "    logprob_after = sentence_logprob(tokens_with)\n",
                "    \n",
                "    delta = logprob_after - logprob_before\n",
                "    return delta > lm_threshold\n",
                "\n",
                "print(\"‚úÖ Language model functions ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "align-md",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Alignment & Insertion Logic\n",
                "\n",
                "The core algorithm:\n",
                "1. Compare clean text with ASR output using `SequenceMatcher`\n",
                "2. Find words that ASR detected but aren't in clean text (insertions)\n",
                "3. For each candidate insertion, check:\n",
                "   - Is it a known disfluency?\n",
                "   - Does the ASR confidence exceed the threshold?\n",
                "   - Does the LM approve the insertion?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "position-prior",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# POSITION PRIOR\n",
                "# =============================================================================\n",
                "\n",
                "def position_prior(token_index, n_tokens, exponent=1.5):\n",
                "    \"\"\"\n",
                "    Bias insertions toward earlier positions in the sentence.\n",
                "    Disfluencies typically occur at the start when speakers are thinking.\n",
                "    \n",
                "    Returns: Score between 0 and 1 (higher = more likely insertion point)\n",
                "    \"\"\"\n",
                "    if n_tokens <= 1:\n",
                "        return 1.0\n",
                "    frac = token_index / float(max(1, n_tokens - 1))\n",
                "    return 1.0 - frac ** exponent\n",
                "\n",
                "print(\"‚úÖ Position prior function ready\")\n",
                "print(f\"   Position 0 (start): {position_prior(0, 10):.2f}\")\n",
                "print(f\"   Position 5 (middle): {position_prior(5, 10):.2f}\")\n",
                "print(f\"   Position 9 (end): {position_prior(9, 10):.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "alignment",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# FIND INSERTIONS (CORE ALGORITHM)\n",
                "# =============================================================================\n",
                "\n",
                "def find_insertions(clean_tokens, asr_tokens, asr_tokens_info=None,\n",
                "                    pos_exponent=1.5, use_lm=True, lm_threshold=-2.0):\n",
                "    \"\"\"\n",
                "    Find disfluencies to insert from ASR output into clean text.\n",
                "    \n",
                "    Algorithm:\n",
                "    1. Align clean and ASR tokens using SequenceMatcher\n",
                "    2. For 'insert' operations (tokens in ASR but not clean):\n",
                "       - Check if it's a known disfluency\n",
                "       - Check if ASR confidence + position prior exceeds threshold\n",
                "       - Check if LM approves the insertion\n",
                "    3. For 'replace' operations, only insert disfluencies\n",
                "    \n",
                "    Returns: List of (position, token) tuples\n",
                "    \"\"\"\n",
                "    if not asr_tokens:\n",
                "        return []\n",
                "    \n",
                "    # Build map: normalized word -> list of token infos\n",
                "    info_map = {}\n",
                "    if asr_tokens_info:\n",
                "        for info in asr_tokens_info:\n",
                "            if info and 'word' in info:\n",
                "                norm_word = normalize_text(info['word'])\n",
                "                info_map.setdefault(norm_word, []).append(info)\n",
                "    \n",
                "    # Find differences between clean and ASR\n",
                "    sm = SequenceMatcher(a=clean_tokens, b=asr_tokens, autojunk=False)\n",
                "    insertions = []\n",
                "    n_clean = len(clean_tokens)\n",
                "    \n",
                "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
                "        if tag == 'insert':\n",
                "            # ASR has tokens that clean text doesn't\n",
                "            for j in range(j1, j2):\n",
                "                token = asr_tokens[j]\n",
                "                norm_token = normalize_text(token)\n",
                "                is_disf = is_disfluency(token)\n",
                "                \n",
                "                # Get ASR confidence for this token\n",
                "                avg_lp = None\n",
                "                if norm_token in info_map and info_map[norm_token]:\n",
                "                    token_info = info_map[norm_token].pop(0)\n",
                "                    avg_lp = token_info.get('avg_logprob')\n",
                "                \n",
                "                # Calculate position score\n",
                "                pos_score = position_prior(i1, n_clean, pos_exponent)\n",
                "                \n",
                "                # Get threshold for this disfluency\n",
                "                conf_threshold = get_disfluency_threshold(token)\n",
                "                \n",
                "                # Decision logic\n",
                "                should_insert = False\n",
                "                \n",
                "                if is_disf:\n",
                "                    # Known disfluency - use per-disfluency threshold\n",
                "                    if avg_lp is not None:\n",
                "                        score = avg_lp + np.log(pos_score + 1e-6)\n",
                "                        should_insert = score > conf_threshold\n",
                "                    else:\n",
                "                        should_insert = True  # No confidence, trust disfluency set\n",
                "                elif is_repetition(token, clean_tokens, i1):\n",
                "                    should_insert = True  # Repetitions are disfluent\n",
                "                elif avg_lp is not None:\n",
                "                    # Non-disfluency: need very high confidence\n",
                "                    score = avg_lp + np.log(pos_score + 1e-6)\n",
                "                    should_insert = score > -4.0\n",
                "                \n",
                "                # LM check\n",
                "                if should_insert and use_lm and NGRAM_COUNTS is not None:\n",
                "                    if not check_insertion_plausibility(clean_tokens, i1, token, lm_threshold):\n",
                "                        should_insert = False\n",
                "                \n",
                "                if should_insert:\n",
                "                    insertions.append((i1, token))\n",
                "        \n",
                "        elif tag == 'replace':\n",
                "            # ASR has different tokens - only insert disfluencies\n",
                "            for j in range(j1, j2):\n",
                "                token = asr_tokens[j]\n",
                "                if is_disfluency(token):\n",
                "                    if use_lm and NGRAM_COUNTS is not None:\n",
                "                        if check_insertion_plausibility(clean_tokens, i1, token, lm_threshold):\n",
                "                            insertions.append((i1, token))\n",
                "                    else:\n",
                "                        insertions.append((i1, token))\n",
                "    \n",
                "    return insertions\n",
                "\n",
                "print(\"‚úÖ Insertion detection function ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "apply-insertions",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# APPLY INSERTIONS\n",
                "# =============================================================================\n",
                "\n",
                "def apply_insertions(original_words, insertions, max_consecutive=4):\n",
                "    \"\"\"\n",
                "    Insert disfluencies into the original text.\n",
                "    \n",
                "    Args:\n",
                "        original_words: List of original words\n",
                "        insertions: List of (position, token) tuples\n",
                "        max_consecutive: Limit consecutive same tokens (prevents \"‡§π‡§Æ‡•ç‡§Æ ‡§π‡§Æ‡•ç‡§Æ ‡§π‡§Æ‡•ç‡§Æ ‡§π‡§Æ‡•ç‡§Æ ‡§π‡§Æ‡•ç‡§Æ\")\n",
                "    \n",
                "    Returns: List of words with insertions\n",
                "    \"\"\"\n",
                "    result = list(original_words)\n",
                "    \n",
                "    # Sort by position descending (so insertions don't shift later positions)\n",
                "    sorted_insertions = sorted(insertions, key=lambda x: (-x[0], insertions.index(x)))\n",
                "    \n",
                "    for pos, token in sorted_insertions:\n",
                "        pos = min(pos, len(result))\n",
                "        \n",
                "        # Count consecutive same tokens\n",
                "        consec = 1\n",
                "        i = pos - 1\n",
                "        while i >= 0 and result[i] == token:\n",
                "            consec += 1\n",
                "            i -= 1\n",
                "        i = pos\n",
                "        while i < len(result) and result[i] == token:\n",
                "            consec += 1\n",
                "            i += 1\n",
                "        \n",
                "        # Only insert if under limit\n",
                "        if consec <= max_consecutive:\n",
                "            result.insert(pos, token)\n",
                "    \n",
                "    return result\n",
                "\n",
                "print(\"‚úÖ Insertion application function ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "restore-main",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# MAIN RESTORATION FUNCTION\n",
                "# =============================================================================\n",
                "\n",
                "def restore_disfluencies(clean_text, audio_path, pos_exponent=1.5, use_lm=True, lm_threshold=-2.0):\n",
                "    \"\"\"\n",
                "    Main function: Restore disfluencies to clean text using audio.\n",
                "    \n",
                "    Args:\n",
                "        clean_text: Text without disfluencies\n",
                "        audio_path: Path to audio file\n",
                "        pos_exponent: Position prior exponent (higher = stronger bias to start)\n",
                "        use_lm: Whether to use language model filtering\n",
                "        lm_threshold: LM threshold (more negative = more lenient)\n",
                "    \n",
                "    Returns: Text with disfluencies restored\n",
                "    \"\"\"\n",
                "    if pd.isna(clean_text) or not isinstance(clean_text, str):\n",
                "        clean_text = \"\"\n",
                "    \n",
                "    # Get ASR transcription\n",
                "    asr_text, asr_tokens_info = get_asr_transcript(audio_path)\n",
                "    \n",
                "    if not asr_text:\n",
                "        return clean_text if clean_text else \"\"\n",
                "    \n",
                "    if not clean_text:\n",
                "        return asr_text\n",
                "    \n",
                "    # Tokenize for alignment\n",
                "    clean_tokens = tokenize(clean_text)\n",
                "    asr_tokens = tokenize(asr_text)\n",
                "    \n",
                "    # Find insertions\n",
                "    insertions = find_insertions(\n",
                "        clean_tokens, asr_tokens,\n",
                "        asr_tokens_info=asr_tokens_info,\n",
                "        pos_exponent=pos_exponent,\n",
                "        use_lm=use_lm,\n",
                "        lm_threshold=lm_threshold\n",
                "    )\n",
                "    \n",
                "    if not insertions:\n",
                "        return clean_text\n",
                "    \n",
                "    # Apply insertions\n",
                "    original_words = clean_text.split()\n",
                "    restored_words = apply_insertions(original_words, insertions)\n",
                "    \n",
                "    return ' '.join(restored_words)\n",
                "\n",
                "print(\"‚úÖ Main restoration function ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "run-md",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Main Inference Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run-inference",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# INFERENCE FUNCTION\n",
                "# =============================================================================\n",
                "\n",
                "def run_inference(pos_exponent=1.5, use_lm=True, lm_threshold=-2.0):\n",
                "    \"\"\"\n",
                "    Process all test samples and generate submission.\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"üöÄ DISFLUENCY RESTORATION PIPELINE\")\n",
                "    print(\"=\" * 60)\n",
                "    print(f\"   Position exponent: {pos_exponent}\")\n",
                "    print(f\"   Use LM: {use_lm}\")\n",
                "    print(f\"   LM threshold: {lm_threshold}\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    # Load cache\n",
                "    load_cache()\n",
                "    \n",
                "    # Build LM if needed\n",
                "    if use_lm and NGRAM_COUNTS is None:\n",
                "        build_ngram_model()\n",
                "    \n",
                "    # Load test data\n",
                "    test_df = pd.read_csv(TEST_CSV)\n",
                "    print(f\"\\nüìä Processing {len(test_df)} test samples...\")\n",
                "    \n",
                "    results = []\n",
                "    for i, row in test_df.iterrows():\n",
                "        audio_path = f\"{AUDIO_DIR}/{row['id']}.wav\"\n",
                "        \n",
                "        if os.path.exists(audio_path):\n",
                "            restored = restore_disfluencies(\n",
                "                row['transcript'], audio_path,\n",
                "                pos_exponent=pos_exponent,\n",
                "                use_lm=use_lm,\n",
                "                lm_threshold=lm_threshold\n",
                "            )\n",
                "        else:\n",
                "            restored = row['transcript']\n",
                "            log(f\"‚ö†Ô∏è Audio not found: {audio_path}\")\n",
                "        \n",
                "        results.append({'id': row['id'], 'transcript': restored})\n",
                "        \n",
                "        # Progress update every 10 samples\n",
                "        if (i + 1) % 10 == 0:\n",
                "            save_cache()\n",
                "            print(f\"   ‚úì Processed {i + 1}/{len(test_df)}\")\n",
                "    \n",
                "    # Save final results\n",
                "    save_cache()\n",
                "    output_path = os.path.join(OUTPUT_DIR, \"submission.csv\")\n",
                "    pd.DataFrame(results).to_csv(output_path, index=False)\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(f\"‚úÖ COMPLETE! Saved to: {output_path}\")\n",
                "    print(f\"   Total samples: {len(results)}\")\n",
                "    print(f\"   Output files: {os.listdir(OUTPUT_DIR)}\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    return results\n",
                "\n",
                "print(\"‚úÖ Inference function ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exec-md",
            "metadata": {},
            "source": [
                "## üèÉ Execute Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "execute",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# RUN THE PIPELINE\n",
                "# =============================================================================\n",
                "\n",
                "results = run_inference(\n",
                "    pos_exponent=1.5,    # Bias toward sentence start\n",
                "    use_lm=True,         # Use language model\n",
                "    lm_threshold=-2.0    # LM threshold\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}