{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# üìä Hindi Disfluency Restoration - Exploratory Data Analysis\n",
                "\n",
                "This notebook analyzes the disfluency patterns in Hindi speech transcripts to guide our restoration strategy.\n",
                "\n",
                "**Sections:**\n",
                "1. Data Loading & Overview\n",
                "2. Disfluency Frequency Analysis\n",
                "3. Position Analysis (Where do disfluencies occur?)\n",
                "4. Count per Sample\n",
                "5. Consecutive Disfluencies\n",
                "6. Text Length Analysis\n",
                "7. Context Analysis (Before/After words)\n",
                "8. Train vs Test Comparison\n",
                "9. Audio Analysis\n",
                "10. Summary & Recommendations\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-md",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup & Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# IMPORTS\n",
                "# =============================================================================\n",
                "\n",
                "import os\n",
                "import re\n",
                "import pickle\n",
                "import unicodedata\n",
                "from collections import Counter\n",
                "from pathlib import Path\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import librosa\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Plotting style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 5)\n",
                "plt.rcParams['font.size'] = 11\n",
                "sns.set_palette('husl')\n",
                "\n",
                "print(\"‚úÖ Imports complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# CONFIGURATION\n",
                "# =============================================================================\n",
                "\n",
                "# Data paths (Kaggle)\n",
                "INPUT_DIR = Path(\"/kaggle/input/nppe-2-automatic-disfluency-restoration\")\n",
                "AUDIO_DIR = INPUT_DIR / \"downloaded_audios\"\n",
                "\n",
                "print(f\"üìÅ Input directory: {INPUT_DIR}\")\n",
                "print(f\"üéµ Audio directory: {AUDIO_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# LOAD DATASETS\n",
                "# =============================================================================\n",
                "\n",
                "# Load CSVs\n",
                "train_df = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
                "test_df = pd.read_csv(INPUT_DIR / \"test.csv\")\n",
                "disf_df = pd.read_csv(INPUT_DIR / \"unique_disfluencies.csv\")\n",
                "\n",
                "# Display summary\n",
                "print(\"=\" * 50)\n",
                "print(\"üìä DATASET OVERVIEW\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\nüîπ Train samples:       {len(train_df):,}\")\n",
                "print(f\"üîπ Test samples:        {len(test_df):,}\")\n",
                "print(f\"üîπ Unique disfluencies: {len(disf_df):,}\")\n",
                "\n",
                "print(\"\\nüìã Train columns:\", list(train_df.columns))\n",
                "print(\"üìã Test columns: \", list(test_df.columns))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "disfluency-set",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# BUILD DISFLUENCY SET (with Unicode variants)\n",
                "# =============================================================================\n",
                "# Hindi has multiple Unicode representations for the same character.\n",
                "# We create variants to catch all possible spellings.\n",
                "\n",
                "DISFLUENCY_SET = set(disf_df['disfluency'].str.strip().tolist())\n",
                "\n",
                "# Add Unicode variants\n",
                "variants = set()\n",
                "for d in DISFLUENCY_SET:\n",
                "    variants.add(unicodedata.normalize('NFC', d))   # Composed form\n",
                "    variants.add(unicodedata.normalize('NFD', d))   # Decomposed form\n",
                "    variants.add(unicodedata.normalize('NFKC', d))  # Compatibility composed\n",
                "    variants.add(d.replace('‡§Ç', '‡§Å'))               # Anusvara ‚Üî Chandrabindu\n",
                "    variants.add(d.replace('‡§Å', '‡§Ç'))\n",
                "\n",
                "DISFLUENCY_SET.update(variants)\n",
                "\n",
                "# Add common fillers that might be missing\n",
                "COMMON_FILLERS = {'‡§π‡§Æ‡•ç‡§Æ', '‡§π‡§æ‡§Ç', '‡§π‡§æ‡§Å', '‡§â‡§Æ‡•ç‡§Æ', '‡§Ö‡§Æ‡•ç‡§Æ', '‡§π', '‡§Ö‡§Ç', '‡§§‡•ã', '‡§µ‡•ã', '‡§Æ‡§§‡§≤‡§¨'}\n",
                "DISFLUENCY_SET.update(COMMON_FILLERS)\n",
                "\n",
                "print(f\"‚úÖ Total disfluencies (with variants): {len(DISFLUENCY_SET)}\")\n",
                "print(f\"\\nüìù Sample disfluencies: {list(DISFLUENCY_SET)[:10]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helpers-md",
            "metadata": {},
            "source": [
                "## üõ†Ô∏è Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# HELPER FUNCTIONS\n",
                "# =============================================================================\n",
                "\n",
                "def extract_disfluencies(text):\n",
                "    \"\"\"\n",
                "    Find all disfluencies in a transcript.\n",
                "    Returns list of {'disfluency': word, 'position': index, 'total_words': n}\n",
                "    \"\"\"\n",
                "    if pd.isna(text):\n",
                "        return []\n",
                "    \n",
                "    words = str(text).split()\n",
                "    found = []\n",
                "    \n",
                "    for i, word in enumerate(words):\n",
                "        # Clean punctuation\n",
                "        clean = word.strip('‡•§‡••,;:!?\\'\"-')\n",
                "        # Normalize for matching\n",
                "        norm = unicodedata.normalize('NFC', clean.lower())\n",
                "        \n",
                "        if norm in DISFLUENCY_SET or clean in DISFLUENCY_SET:\n",
                "            found.append({\n",
                "                'disfluency': norm,\n",
                "                'position': i,\n",
                "                'total_words': len(words)\n",
                "            })\n",
                "    \n",
                "    return found\n",
                "\n",
                "\n",
                "def find_consecutive_runs(disfluencies):\n",
                "    \"\"\"\n",
                "    Find runs of consecutive disfluencies (e.g., \"‡§π‡§Æ‡•ç‡§Æ ‡§π‡§æ‡§Ç ‡§â‡§Æ‡•ç‡§Æ\").\n",
                "    Returns list of run lengths.\n",
                "    \"\"\"\n",
                "    if not disfluencies:\n",
                "        return []\n",
                "    \n",
                "    positions = sorted([d['position'] for d in disfluencies])\n",
                "    runs = []\n",
                "    current_run = 1\n",
                "    \n",
                "    for i in range(1, len(positions)):\n",
                "        if positions[i] == positions[i-1] + 1:\n",
                "            current_run += 1\n",
                "        else:\n",
                "            if current_run > 1:\n",
                "                runs.append(current_run)\n",
                "            current_run = 1\n",
                "    \n",
                "    if current_run > 1:\n",
                "        runs.append(current_run)\n",
                "    \n",
                "    return runs\n",
                "\n",
                "\n",
                "def get_context(text, disfluencies):\n",
                "    \"\"\"\n",
                "    Get words before and after each disfluency.\n",
                "    Useful for understanding context patterns.\n",
                "    \"\"\"\n",
                "    if pd.isna(text) or not disfluencies:\n",
                "        return []\n",
                "    \n",
                "    words = str(text).split()\n",
                "    contexts = []\n",
                "    \n",
                "    for d in disfluencies:\n",
                "        pos = d['position']\n",
                "        before = words[pos-1] if pos > 0 else '<START>'\n",
                "        after = words[pos+1] if pos < len(words)-1 else '<END>'\n",
                "        \n",
                "        contexts.append({\n",
                "            'disfluency': d['disfluency'],\n",
                "            'before': before,\n",
                "            'after': after\n",
                "        })\n",
                "    \n",
                "    return contexts\n",
                "\n",
                "\n",
                "print(\"‚úÖ Helper functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "freq-md",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Disfluency Frequency Analysis\n",
                "\n",
                "Which disfluencies are most common? This helps us prioritize which ones to focus on."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "extract-all",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# EXTRACT ALL DISFLUENCIES FROM TRAINING DATA\n",
                "# =============================================================================\n",
                "\n",
                "print(\"üîÑ Extracting disfluencies from training data...\")\n",
                "\n",
                "# Apply extraction to all transcripts\n",
                "train_df['disfluencies'] = train_df['transcript'].apply(extract_disfluencies)\n",
                "train_df['disf_count'] = train_df['disfluencies'].apply(len)\n",
                "\n",
                "# Flatten all disfluencies into one list\n",
                "all_disfluencies = []\n",
                "for disfs in train_df['disfluencies']:\n",
                "    all_disfluencies.extend(disfs)\n",
                "\n",
                "# Convert to DataFrame for analysis\n",
                "disf_analysis = pd.DataFrame(all_disfluencies)\n",
                "\n",
                "print(f\"‚úÖ Found {len(disf_analysis):,} total disfluency occurrences\")\n",
                "print(f\"   in {len(train_df):,} transcripts\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "freq-plot",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# FREQUENCY ANALYSIS PLOT\n",
                "# =============================================================================\n",
                "\n",
                "disf_freq = disf_analysis['disfluency'].value_counts()\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Absolute counts\n",
                "disf_freq.head(15).plot(kind='barh', ax=axes[0], color='steelblue', edgecolor='black')\n",
                "axes[0].set_title('üîπ Top 15 Most Common Disfluencies', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xlabel('Count')\n",
                "axes[0].invert_yaxis()  # Highest at top\n",
                "\n",
                "# Plot 2: Percentage of total\n",
                "disf_pct = (disf_freq.head(15) / len(disf_analysis) * 100)\n",
                "disf_pct.plot(kind='barh', ax=axes[1], color='darkorange', edgecolor='black')\n",
                "axes[1].set_title('üîπ Top 15 Disfluencies (% of Total)', fontsize=12, fontweight='bold')\n",
                "axes[1].set_xlabel('Percentage (%)')\n",
                "axes[1].invert_yaxis()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Key insight\n",
                "top5_coverage = disf_freq.head(5).sum() / len(disf_analysis) * 100\n",
                "print(f\"\\nüìä KEY INSIGHT: Top 5 disfluencies cover {top5_coverage:.1f}% of all occurrences!\")\n",
                "print(f\"   Top 5: {disf_freq.head(5).index.tolist()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "pos-md",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Position Analysis\n",
                "\n",
                "Where in the sentence do disfluencies occur? This helps us set position priors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "position-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# POSITION ANALYSIS\n",
                "# =============================================================================\n",
                "\n",
                "# Calculate relative position (0 = start, 1 = end)\n",
                "disf_analysis['relative_position'] = disf_analysis['position'] / disf_analysis['total_words'].clip(lower=1)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Position distribution histogram\n",
                "axes[0].hist(disf_analysis['relative_position'], bins=20, edgecolor='black', alpha=0.7, color='teal')\n",
                "axes[0].set_title('üîπ Disfluency Position Distribution', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xlabel('Relative Position (0=start, 1=end)')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].axvline(x=0.25, color='red', linestyle='--', linewidth=2, label='First Quarter')\n",
                "axes[0].legend()\n",
                "\n",
                "# Plot 2: Mean position by disfluency type\n",
                "top_disfs = disf_freq.head(8).index.tolist()\n",
                "position_by_type = disf_analysis[disf_analysis['disfluency'].isin(top_disfs)].groupby('disfluency')['relative_position'].mean()\n",
                "position_by_type.sort_values().plot(kind='barh', ax=axes[1], color='purple', edgecolor='black')\n",
                "axes[1].set_title('üîπ Mean Position by Disfluency Type', fontsize=12, fontweight='bold')\n",
                "axes[1].set_xlabel('Mean Relative Position')\n",
                "axes[1].axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Middle')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Statistics\n",
                "first_quarter = (disf_analysis['relative_position'] < 0.25).sum() / len(disf_analysis) * 100\n",
                "at_start = (disf_analysis['position'] == 0).sum() / len(disf_analysis) * 100\n",
                "\n",
                "print(f\"\\nüìä KEY INSIGHTS:\")\n",
                "print(f\"   ‚Ä¢ {first_quarter:.1f}% of disfluencies occur in the first quarter\")\n",
                "print(f\"   ‚Ä¢ {at_start:.1f}% of disfluencies are at sentence start\")\n",
                "print(f\"   ‚üπ Use position prior biased toward start!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "count-md",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Disfluency Count per Sample\n",
                "\n",
                "How many disfluencies does each transcript have?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "count-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# COUNT PER SAMPLE\n",
                "# =============================================================================\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Histogram of counts\n",
                "train_df['disf_count'].hist(bins=range(0, 15), ax=axes[0], edgecolor='black', alpha=0.7, color='coral')\n",
                "axes[0].set_title('üîπ Disfluencies per Transcript', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xlabel('Number of Disfluencies')\n",
                "axes[0].set_ylabel('Number of Transcripts')\n",
                "axes[0].set_xticks(range(0, 15))\n",
                "\n",
                "# Plot 2: Cumulative percentage\n",
                "counts = train_df['disf_count'].value_counts().sort_index()\n",
                "cumsum = counts.cumsum() / len(train_df) * 100\n",
                "cumsum.plot(ax=axes[1], marker='o', color='purple', linewidth=2)\n",
                "axes[1].set_title('üîπ Cumulative % of Samples', fontsize=12, fontweight='bold')\n",
                "axes[1].set_xlabel('Max Disfluencies per Sample')\n",
                "axes[1].set_ylabel('Cumulative %')\n",
                "axes[1].axhline(y=90, color='red', linestyle='--', label='90% coverage')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Statistics\n",
                "print(f\"\\nüìä STATISTICS:\")\n",
                "print(f\"   ‚Ä¢ Mean disfluencies/sample: {train_df['disf_count'].mean():.2f}\")\n",
                "print(f\"   ‚Ä¢ Median: {train_df['disf_count'].median():.0f}\")\n",
                "print(f\"   ‚Ä¢ Max: {train_df['disf_count'].max()}\")\n",
                "print(f\"   ‚Ä¢ Samples with 0 disfluencies: {(train_df['disf_count'] == 0).sum()} ({(train_df['disf_count'] == 0).mean()*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "consec-md",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Consecutive Disfluencies\n",
                "\n",
                "Do disfluencies appear in runs (e.g., \"‡§π‡§Æ‡•ç‡§Æ ‡§π‡§æ‡§Ç ‡§â‡§Æ‡•ç‡§Æ\")?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "consecutive",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# CONSECUTIVE DISFLUENCIES\n",
                "# =============================================================================\n",
                "\n",
                "# Find consecutive runs in each transcript\n",
                "train_df['consecutive_runs'] = train_df['disfluencies'].apply(find_consecutive_runs)\n",
                "train_df['max_consecutive'] = train_df['consecutive_runs'].apply(lambda x: max(x) if x else 0)\n",
                "\n",
                "# Flatten all runs\n",
                "all_runs = [run for runs in train_df['consecutive_runs'] for run in runs]\n",
                "\n",
                "print(f\"üìä CONSECUTIVE DISFLUENCY ANALYSIS:\")\n",
                "print(f\"   ‚Ä¢ Samples with consecutive disfluencies: {(train_df['max_consecutive'] > 0).sum()}\")\n",
                "print(f\"   ‚Ä¢ Max consecutive in single sample: {train_df['max_consecutive'].max()}\")\n",
                "\n",
                "if all_runs:\n",
                "    print(f\"   ‚Ä¢ Mean run length: {np.mean(all_runs):.2f}\")\n",
                "    \n",
                "    plt.figure(figsize=(8, 4))\n",
                "    pd.Series(all_runs).value_counts().sort_index().plot(kind='bar', color='coral', edgecolor='black')\n",
                "    plt.title('üîπ Consecutive Disfluency Run Lengths', fontsize=12, fontweight='bold')\n",
                "    plt.xlabel('Run Length')\n",
                "    plt.ylabel('Count')\n",
                "    plt.xticks(rotation=0)\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"   ‚ö†Ô∏è No consecutive disfluencies found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "length-md",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Text Length Analysis\n",
                "\n",
                "Does transcript length correlate with disfluency count?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "length-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# TEXT LENGTH ANALYSIS\n",
                "# =============================================================================\n",
                "\n",
                "train_df['word_count'] = train_df['transcript'].fillna('').apply(lambda x: len(x.split()))\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Word count distribution\n",
                "train_df['word_count'].hist(bins=50, ax=axes[0], edgecolor='black', alpha=0.7, color='skyblue')\n",
                "axes[0].set_title('üîπ Word Count Distribution', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xlabel('Number of Words')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].axvline(x=train_df['word_count'].mean(), color='red', linestyle='--', label=f'Mean: {train_df[\"word_count\"].mean():.1f}')\n",
                "axes[0].legend()\n",
                "\n",
                "# Plot 2: Disfluencies vs Word count (scatter)\n",
                "axes[1].scatter(train_df['word_count'], train_df['disf_count'], alpha=0.3, s=15, color='steelblue')\n",
                "axes[1].set_title('üîπ Disfluencies vs Word Count', fontsize=12, fontweight='bold')\n",
                "axes[1].set_xlabel('Word Count')\n",
                "axes[1].set_ylabel('Disfluency Count')\n",
                "\n",
                "# Add trend line\n",
                "z = np.polyfit(train_df['word_count'], train_df['disf_count'], 1)\n",
                "p = np.poly1d(z)\n",
                "x_line = np.linspace(0, train_df['word_count'].max(), 100)\n",
                "axes[1].plot(x_line, p(x_line), 'r--', linewidth=2, label=f'Trend: {z[0]:.3f}x + {z[1]:.1f}')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Correlation\n",
                "corr = train_df['word_count'].corr(train_df['disf_count'])\n",
                "print(f\"\\nüìä CORRELATION:\")\n",
                "print(f\"   ‚Ä¢ Word count ‚Üî Disfluency count: r = {corr:.3f}\")\n",
                "print(f\"   ‚üπ {'Moderate positive' if corr > 0.3 else 'Weak'} correlation\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "context-md",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Context Analysis\n",
                "\n",
                "What words typically appear before/after disfluencies?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "context-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# CONTEXT ANALYSIS\n",
                "# =============================================================================\n",
                "\n",
                "print(\"üîÑ Extracting context for each disfluency...\")\n",
                "\n",
                "all_contexts = []\n",
                "for _, row in train_df.iterrows():\n",
                "    all_contexts.extend(get_context(row['transcript'], row['disfluencies']))\n",
                "\n",
                "context_df = pd.DataFrame(all_contexts)\n",
                "\n",
                "if len(context_df) > 0:\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    # Words BEFORE disfluencies\n",
                "    before_words = context_df[context_df['before'] != '<START>']['before'].value_counts().head(15)\n",
                "    before_words.plot(kind='barh', ax=axes[0], color='lightcoral', edgecolor='black')\n",
                "    axes[0].set_title('üîπ Most Common Words BEFORE Disfluencies', fontsize=12, fontweight='bold')\n",
                "    axes[0].set_xlabel('Count')\n",
                "    axes[0].invert_yaxis()\n",
                "    \n",
                "    # Words AFTER disfluencies\n",
                "    after_words = context_df[context_df['after'] != '<END>']['after'].value_counts().head(15)\n",
                "    after_words.plot(kind='barh', ax=axes[1], color='lightgreen', edgecolor='black')\n",
                "    axes[1].set_title('üîπ Most Common Words AFTER Disfluencies', fontsize=12, fontweight='bold')\n",
                "    axes[1].set_xlabel('Count')\n",
                "    axes[1].invert_yaxis()\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Statistics\n",
                "    start_pct = (context_df['before'] == '<START>').sum() / len(context_df) * 100\n",
                "    print(f\"\\nüìä {start_pct:.1f}% of disfluencies occur at sentence start\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No context data available\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "compare-md",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Train vs Test Comparison\n",
                "\n",
                "Are train and test distributions similar?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train-test-compare",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# TRAIN VS TEST DISTRIBUTION\n",
                "# =============================================================================\n",
                "\n",
                "test_df['word_count'] = test_df['transcript'].fillna('').apply(lambda x: len(x.split()))\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "\n",
                "train_df['word_count'].hist(bins=50, alpha=0.5, label='Train', ax=ax, density=True, color='steelblue')\n",
                "test_df['word_count'].hist(bins=50, alpha=0.5, label='Test', ax=ax, density=True, color='coral')\n",
                "\n",
                "ax.set_title('üîπ Word Count Distribution: Train vs Test', fontsize=12, fontweight='bold')\n",
                "ax.set_xlabel('Word Count')\n",
                "ax.set_ylabel('Density')\n",
                "ax.legend(fontsize=11)\n",
                "\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüìä COMPARISON:\")\n",
                "print(f\"   Train - Mean: {train_df['word_count'].mean():.1f}, Median: {train_df['word_count'].median():.0f}\")\n",
                "print(f\"   Test  - Mean: {test_df['word_count'].mean():.1f}, Median: {test_df['word_count'].median():.0f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "audio-md",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Audio Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "audio-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# AUDIO FILE ANALYSIS\n",
                "# =============================================================================\n",
                "\n",
                "# Check which audio files exist\n",
                "train_df['audio_exists'] = train_df['id'].apply(lambda x: (AUDIO_DIR / f\"{x}.wav\").exists())\n",
                "\n",
                "print(f\"üìä AUDIO AVAILABILITY:\")\n",
                "print(f\"   ‚Ä¢ Audio available: {train_df['audio_exists'].sum()}/{len(train_df)} ({train_df['audio_exists'].mean()*100:.1f}%)\")\n",
                "\n",
                "# Sample audio duration analysis (if audio exists)\n",
                "if train_df['audio_exists'].any():\n",
                "    sample = train_df[train_df['audio_exists']].sample(min(50, train_df['audio_exists'].sum()))\n",
                "    durations = []\n",
                "    \n",
                "    print(\"\\nüîÑ Analyzing sample audio durations...\")\n",
                "    for _, row in tqdm(sample.iterrows(), total=len(sample)):\n",
                "        try:\n",
                "            duration = librosa.get_duration(path=str(AUDIO_DIR / f\"{row['id']}.wav\"))\n",
                "            durations.append({\n",
                "                'id': row['id'],\n",
                "                'duration': duration,\n",
                "                'word_count': row['word_count']\n",
                "            })\n",
                "        except Exception as e:\n",
                "            pass\n",
                "    \n",
                "    if durations:\n",
                "        dur_df = pd.DataFrame(durations)\n",
                "        speaking_rate = dur_df['word_count'].sum() / dur_df['duration'].sum()\n",
                "        \n",
                "        print(f\"\\nüìä AUDIO STATISTICS (sample of {len(durations)}):\")\n",
                "        print(f\"   ‚Ä¢ Mean duration: {dur_df['duration'].mean():.1f} seconds\")\n",
                "        print(f\"   ‚Ä¢ Speaking rate: {speaking_rate:.1f} words/second\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è No audio files found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary-md",
            "metadata": {},
            "source": [
                "## üîü Summary & Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "summary",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# SUMMARY & RECOMMENDATIONS\n",
                "# =============================================================================\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"üìà KEY STATISTICS SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "print(f\"\\nüîπ Total disfluencies in training: {len(disf_analysis):,}\")\n",
                "print(f\"üîπ Mean disfluencies per sample:   {train_df['disf_count'].mean():.2f}\")\n",
                "print(f\"üîπ Top 5 disfluencies cover:       {top5_coverage:.1f}% of all\")\n",
                "print(f\"üîπ In first quarter of sentence:   {first_quarter:.1f}%\")\n",
                "print(f\"üîπ At sentence start:              {at_start:.1f}%\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"üí° RECOMMENDATIONS FOR PIPELINE\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "recommendations = [\n",
                "    (\"POSITION PRIOR\", \"Bias insertions toward sentence start (first 25%)\"),\n",
                "    (\"TOP-K FOCUS\", \"Prioritize top 5-10 disfluencies for higher precision\"),\n",
                "    (\"CONSECUTIVE\", \"Handle back-to-back disfluencies (limit max 4)\"),\n",
                "    (\"CONTEXT PATTERNS\", \"Use before/after word patterns for validation\"),\n",
                "    (\"PER-DISFLUENCY THRESHOLDS\", \"Different confidence thresholds per disfluency type\"),\n",
                "]\n",
                "\n",
                "for i, (title, desc) in enumerate(recommendations, 1):\n",
                "    print(f\"\\n{i}. {title}\")\n",
                "    print(f\"   {desc}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(f\"üìä Top 10 Disfluencies: {disf_freq.head(10).index.tolist()}\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "validation-md",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üîç Data Validation\n",
                "\n",
                "to check for data quality issues before running the pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "validation",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# DATA VALIDATION\n",
                "# =============================================================================\n",
                "\n",
                "def validate_dataset(df, audio_dir, name=\"Dataset\"):\n",
                "    \"\"\"Check for common data quality issues.\"\"\"\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"üîç VALIDATING: {name}\")\n",
                "    print(f\"{'='*50}\")\n",
                "    \n",
                "    issues = []\n",
                "    \n",
                "    # Check for null/empty transcripts\n",
                "    null_count = df['transcript'].isna().sum()\n",
                "    empty_count = (df['transcript'].astype(str).str.strip() == '').sum()\n",
                "    \n",
                "    print(f\"\\nüìù Transcripts:\")\n",
                "    print(f\"   ‚Ä¢ Null: {null_count}\")\n",
                "    print(f\"   ‚Ä¢ Empty: {empty_count}\")\n",
                "    \n",
                "    if null_count > 0:\n",
                "        issues.append(f\"{null_count} null transcripts\")\n",
                "    \n",
                "    # Check for missing audio\n",
                "    df['has_audio'] = df['id'].apply(lambda x: (audio_dir / f\"{x}.wav\").exists())\n",
                "    missing_audio = (~df['has_audio']).sum()\n",
                "    \n",
                "    print(f\"\\nüéµ Audio:\")\n",
                "    print(f\"   ‚Ä¢ Missing: {missing_audio}/{len(df)}\")\n",
                "    \n",
                "    if missing_audio > 0:\n",
                "        issues.append(f\"{missing_audio} missing audio files\")\n",
                "    \n",
                "    # Check for duplicates\n",
                "    dup_count = df['id'].duplicated().sum()\n",
                "    print(f\"\\nüÜî IDs:\")\n",
                "    print(f\"   ‚Ä¢ Duplicates: {dup_count}\")\n",
                "    \n",
                "    if dup_count > 0:\n",
                "        issues.append(f\"{dup_count} duplicate IDs\")\n",
                "    \n",
                "    # Summary\n",
                "    if issues:\n",
                "        print(f\"\\n‚ö†Ô∏è Issues found: {', '.join(issues)}\")\n",
                "    else:\n",
                "        print(f\"\\n‚úÖ No issues found!\")\n",
                "    \n",
                "    return issues\n",
                "\n",
                "# Run validation\n",
                "train_issues = validate_dataset(train_df, AUDIO_DIR, \"Training Set\")\n",
                "test_issues = validate_dataset(test_df, AUDIO_DIR, \"Test Set\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"‚úÖ VALIDATION COMPLETE\")\n",
                "print(\"=\" * 50)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
